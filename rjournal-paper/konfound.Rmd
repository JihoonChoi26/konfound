---
title: konfound
author:
  - name: Joshua M. Rosenberg
    affiliation: University of Tennessee, Knoxville
    address:
    - line 1
    - line 2
    email:  jmrosenberg@utk.edu
  - name: Ran Xu
    affiliation: Virginia Tech
    address:
    - line 1
    - line 2
    email:  ranxu@msu.edu
  - name: Kenneth A. Frank
    affiliation: Michigan State University
    address:
    - line 1
    - line 2
    email:  kenfrank@msu.edu
abstract: >
  An abstract of less than 150 words.
output:
  rticles::rjournal_article:
    includes:
      in_header: preamble.tex
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE,
comment = "#>",
out.width="70%",
fig.align="center",
tidy=TRUE
)

library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

```

## Introduction

In social science (and educational) research, we often wish to understand how robust inferences about effects are to unobserved (or controlled for) covariates, possible problems with measurement, and other sources of bias.  The goal of `konfound` is to carry out sensitivity analysis to help analysts to *quantify how robust inferences are to potential sources of bias*. This package provides functions based on developments in sensitivity analysis by Frank and colleagues, which previously have been implemented in `Stata` and through an Excel spreadsheet, in `R` through the `konfound` package. 

# Background on sensitivity analysis

<!-- Statistical inferences are often challenged on the basis of uncontrolled bias.  There may be bias due to uncontrolled confounding variables or non-random selection into a sample. Methods for sensitivity analysis have been developed to assess the robustness of inferences to various source of bias and inform debate about causal inference. However, most of the previous methods either only accounted for particular sources of bias (e.g., an unobserved variable) or only applied to certain types of data (e.g. categorical treatment variable; Diprete and Gangl 2004;  Gill and Robins 2001; Robins 1987; Robins, Rotnisky and Scharfstein, 2000; Rosenbaum 1986, 2002; Scharfstein, 2002; Vanderweele 2010, 2011). In a series of papers, Frank and colleagues (2000, 2004, 2007, 2013) have extended previous work and developed two robustness analysis frameworks.  The first uses Rubin’s causal model to interpret how much bias there must be to invalidate an inference in terms of replacing observed cases with counterfactual cases or cases from an unsampled population. The second quantifies the robustness of causal inferences in terms of correlations associated with unobserved variables in a regression framework. -->

<!-- In this paper we introduce the “konfound” package that implements the two robustness analysis methods described above in STATA. Specifically, the konfound command can be used to implement the robustness analysis for the user’s model; the mkonfound command can be used to implement the robustness analysis for multiple studies; and the pkonfound command can be used to implement the robustness analysis for single published study. Next, we briefly discuss the foundations of these two methods, and describe how to use the “konfound” package in STATA. For a longer introduction of the methods and more technical details readers should refer to Frank (2000), Pan and Frank (2004), Frank and Min (2007), Frank et al. (2008, 2013). -->

## Impact threshold of a confounding variable

<!-- This type of robustness analysis concerns the robustness of the inference by expressing concerns about bias in terms of characteristics of unobserved data, as defined by Rubin’s causal model (RCM; Rubin, 1974). In particular, Frank et al. (2013) use RCM to characterize how one could invalidate inferences by replacing observed cases with unobserved cases in which there was no treatment effect. This framework enables researchers to identify a “switch point” (Behn & Vaupel, 1982) where the bias is large enough to undo one’s belief about an effect (e.g., from inferring an effect to inferring no effect). Using the switch point, this framework addresses the concerns pertaining to external validity, such as the extent to which the sampling process has to be biased to invalidate the inference, or concerns pertaining to internal validity, such as the extent to which bias due to uncontrolled preexisting differences can invalidate the inference of the treatment effect. -->
<!-- The sensitivity analysis assesses the robustness of the inference by comparing the estimated effects with a given threshold. The threshold defines the point at which evidence from a study would make one indifferent to the choices. The threshold can be the effect size, or be defined by statistical significance, etc. (Frank et al, 2013).  -->
<!-- One can compare an estimate with a threshold to represent how much bias there must be to switch the inference. For example, consider figure 2, in which the treatment effect from Hypothetical Studies A (with an estimated effect of 6) and B (with an estimated effect of 8) each exceed the threshold of 4. And as the estimated effect from Study B exceeds the threshold by more than does the estimate from Study A (assuming that the estimates were obtained with similar levels of control for selection bias in the design of the study and similar levels of precision), the inference from Study B is more robust than that from Study A because a greater proportion of the estimate from Study B must be due to bias to invalidate the inference. -->

## Percent bias to invalidate an inference

<!-- In observational studies and quasi-experiments, a key concern pertaining to causal inference is the omitted variable bias problem. That is, there are some unobserved confounding variables that may be correlated with both the outcome and the predictor of interest, which will bias the estimates of the model and thus invalidate inferences. To quantify the impact of a confounding variable necessary to alter a statistical inference, Frank (2000) defined the impact of a confounding variable as rx cvr ycv, where rx cv  is the correlation between the unobserved confound and the predictor of interest and r ycv, is the correlation between the unobserved confound and the outcome. For example, if the relationship of interest is between Father’s occupation (X) and One’s own educational attainment (Y), a confounding variable could be Father’s education (cv). And the index developed by Frank (2000) allows us to quantify the impact of father’s education in terms of its correlation with the predictor father’s occupation, as well as its correlation with the outcome – educational attainment. Then Frank (2000) shows how to assess how strong the confounding variable (cv) has to correlate with the predictor (X) as well as the outcome (Y) to invalidate an inference of an effect of X on Y. -->

# Tutorial

You can install `konfound` with the following:

```{r, installation, eval = FALSE}
install.packages("konfound")
```

You can then load konfound with the `library()` function:

```{r, eval = TRUE}
library(konfound)
```

## Example 1: Use of pkonfound() for values from an already-conducted analysis

`pkonfound()` is used when we have values from an already-conducted analysis (like a regression analysis), such as one in an already-published study or from an analysis carried out using other software. 

In the case of a regression analysis, values from the analysis would simply be used as the inputs to the `pkonfound()` function. For example, in the use below, we simply enter the values for the estimated effect (an unstandardardized beta coefficient) (`2`), its standard error (`.4`), the sample size (`100`), and the number of covariates (`3`):

```{r, linewidth = 80}
pkonfound(2, .4, 100, 3)
```

For this set of values, around 60% would need to be false due to a source of bias for the inference to be invalidated (based on statistical significance and a p-value (or alpha) of .05), possible a very robust effect. An omitted, confounding variable (sometimes referred to as a covariate) would need to have an impact (defined as the product of the confounding variable's correlation with both the predictor of interest and the outcome) of 0.323, presenting a different interpretation of how robust this (hypothetical) effect is to a variable which is important but not included in the analysis. 

Here is another example, but one in which the unstandardized beta coefficient is smaller than its standard error:

```{r, linewidth = 80}
pkonfound(.4, 2, 100, 3)
```

Note that this use of `pkonfound()` is equivalent to naming the arguments, i.e. for a different set of values:

```{r, linewidth = 80}
pkonfound(est_eff = -2.2,
          std_err = .65, 
          n_obs = 200,
          n_covariates = 3)
```

We notice that the output includes a message that says we can view other forms of output by changing the `to_return` argument. Here are the two plots - for the bias necessary to alter an inference (`thresh_plot`) and for the robustness of an inference in terms of the impact of a confounding variable (`corr_plot`) that can be returned:

```{r, linewidth = 80}
pkonfound(.4, 2, 100, 3, to_return = "thresh_plot")
```

```{r, linewidth = 80}
pkonfound(.4, 2, 100, 3, to_return = "corr_plot")
```

You can also specify multiple forms of output at once. 

```{r, linewidth = 80}
model_output <- pkonfound(2, .4, 200, 3, to_return = c("raw_output", "thresh_plot", "corr_plot"))
summary(model_output)
```

When we type the name of the object, we see that we created three types of output that we can access as follows:

```{r}
model_output$raw_output
model_output$thresh_plot
model_output$corr_plot
```

Finally, you can return the raw output, for use in other analyses. 

```{r, linewidth = 80}
pkonfound(.4, 2, 100, 3, to_return = "raw_output")
```

## Use of konfound() for models fit in R

Where `pkonfound()` can be used with values from already-conducted analyses, `konfound()` can be used with models (`lm()`, `glm()`, and `lme4::lmer()`) fit in R.

### Example 2A: For linear models fit with lm()

```{r}
m1 <- lm(mpg ~ wt + hp + qsec, data = mtcars)
m1

konfound(m1, hp)
```

Like with `pkonfound()`, we can also output multiple forms of output at once with `konfound()`:

```{r}
konfound_output <- konfound(m1, hp, to_return = c("raw_output", "thresh_plot", "corr_plot"))
summary(konfound_output)
```

Again, we can type each of those, i.e.:

```{r}
konfound_output$raw_output
konfound_output$thresh_plot
```

We can also test all of the variables as predictors of interest:

```{r}
konfound(m1, wt, test_all = TRUE)
```

Whereas this cannot be carried out with `pkonfound()`, with `konfound()` you can also return a table with some key output from the correlation-based approach.

```{r}
konfound(m1, wt, to_return = "table")
```

If the impact threshhold is greater than the impacts of the `Z`s (the other covariates) then an omitted variable would have to have a greater impact than any of the observed covariates to change the inference. Note that in fields in which there is a lot known about covariates given the outcome of interest, then the omitted ones are likely less important than those that are known an included (i.e., we have a good sense of the factors that matter in terms of educational achievement).

### Example 2B: For generalized linear models fit with glm()

Effects for these models are interpreted on the basis of average partial (or marginal) effects (calculated using the `margins` package).

```{r, message = F}
# if forcats is not installed, this install it first using install.packages("forcats") for this to run
if (requireNamespace("forcats")) {
    d <- forcats::gss_cat
    
    d$married <- ifelse(d$marital == "Married", 1, 0)
    
    m2 <- glm(married ~ age, data = d, family = binomial(link = "logit"))
    konfound(m2, age)
}
```

As with models fit with `lm()` (and use of `pkonfound()`), multiple forms of output can be specified with the `to_return` argument to `konfound()`, i.e. `konfound(m2, age, to_return = c("raw_output", "corr_plot", "thresh_plot"))`.

### Example 2C: For mixed effects (or multi-level) models fit with the lmer() function from the lme4 package

`konfound` also works with models fit with the `lmer()` function from the package `lme4`, for mixed-effects or multi-level models. One challenge with carrying out sensitivity analysis for fixed effects in mixed effects models is calculating the correct denominator degrees of freedom for the t-test associated with the coefficients. This is not unique to sensitivity analysis, as, for example, `lmer()` does not report degrees of freedom (or p-values) for fixed effects predictors (see this information in the `lme4` FAQ [here](http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#why-doesnt-lme4-display-denominator-degrees-of-freedomp-values-what-other-options-do-i-have)). While it may be possible to determine the correct degrees of freedom for some models (i.e., models with relatively simple random effects structures), it is difficult to generalize this approach, and so in this package the Kenward-Roger approximation for the denominator degrees of freedom as implemented in the `pbkrtest` package (described in [Halekoh and Højsgaard, 2014](https://www.jstatsoft.org/htaccess.php?volume=59&type=i&issue=09&paper=true)).

Here is an example of the use of `konfound()` with a model fit with `lmer()`:

```{r}
if (requireNamespace("lme4")) {
    library(lme4)
    m3 <- fm1 <- lmer(Reaction ~ Days + (1 | Subject), sleepstudy)
    konfound(m3, Days)
}
```

### Example 2D: Use of mkonfound() for meta-analyses that include sensitivity analysis

We can also use `konfound` to carry out sensitivity analysis as part of meta-analyses. For example, here, `d` represents output from a number (30 in this case) of past studies, read in a CSV file from a website:

```{r}
d <- read.csv("https://msu.edu/~kenfrank/example%20dataset%20for%20mkonfound.csv")
head(d)
mkonfound(d, t, df)
```

We can also return a plot summarizing the percent bias needed to sustan or invalidate an inference across all of the past studies:

```{r}
mkonfound(d, t, df, return_plot = T)
```

# Examples of publishable write-ups

(from Beymer)
(for correlation-based approach)
(ref appendix)

# References

Behn, R.D. and Vaupel, J.W., 1982. Quick analysis for busy decision makers (p. 3). New York: Basic Books.
Cohen, J., Cohen, P., West, S.G. and Aiken, L.S., 1983. Applied multiple regression for the behavioral sciences. Laurence Erlbaum, Hillsdale, NJ.
DiPrete, T.A. and Gangl, M., 2004. Assessing bias in the estimation of causal effects: Rosenbaum bounds on matching estimators and instrumental variables estimation with imperfect instruments. Sociological methodology, 34(1), pp.271-310.
Frank, K. A. and Min, K. 2007. Indices of Robustness for Sample Representation. Sociological Methodology.  Vol 37, 349-392. 
Frank, K.A. 2000. Impact of a Confounding Variable on the Inference of a Regression Coefficient. Sociological Methods and Research, 29(2), 147-194
Frank, K.A., Gary Sykes, Dorothea Anagnostopoulos, Marisa Cannata, Linda Chard, Ann Krause, Raven McCrory. 2008. Extended Influence: National Board Certified Teachers as Help Providers.  Education, Evaluation, and Policy Analysis.  Vol 30(1): 3-3
Frank, K.A., Maroulis, S., Duong, M., and Kelcey, B. 2013.  What would it take to Change an Inference?: Using Rubin’s Causal Model to Interpret the Robustness of Causal Inferences.  Education, Evaluation and Policy Analysis.  Vol 35: 437-460.
Gill, R.D. and Robins, J.M., 2001. Causal inference for complex longitudinal data: the continuous case. Annals of Statistics, pp.1785-1811.
Hamilton, L.C., 1983. Saving water: a causal model of household conservation. Sociological Perspectives, 26(4), pp.355-374.
Hamilton, L.C., 1985. Concern about toxic wastes: Three demographic predictors. Sociological Perspectives, 28(4), pp.463-486.
Hamilton, L.C., 1992. Regression with graphics: A second course in applied statistics.
Pan, W., and Frank, K.A. 2004. An Approximation to the Distribution of the Product of Two Dependent Correlation Coefficients. Journal of Statistical Computation and Simulation, 74, 419-443
Pan, W., and Frank, K.A., 2004. A probability index of the robustness of a causal inference. Journal of Educational and Behavioral Statistics, 28, 315-337.
Robins, J. 1987. “A Graphical Approach to the Identification and Estimation of Causal Parameters inMortality Studies with Sustained Exposure Periods.” Journal of Chronic Diseases 40(2):1395–1615.
Robins, J., A. Rotnisky, and D. Scharfstein. 2000. “Sensitivity Analysis for Selection Bias and Unmeasured Confounding in Missing Data and Causal Inference Models.” Pp. 1–95 in Statistical Models in Epidemiology,the Environment and Clinical Trials (The IMA Volumes in Mathematics and Its Applications), edited by E. Halloran and D. Berry. New York: Springer-Verlag.
Rosenbaum, P. R. 1986. “Dropping Out of High School in the United States: An Observational Study.” Journal of Educational Statistics 11(3):207–24. 
Rosembaum, P. R. 2002. “AttributionEffects to Treatment inMatched Observational Studies.” Journal of the American Statistical Association 97:183–92.
Rubin, D. B. 1974. “Estimating Causal Effects of Treatments in Randomized and Nonrandomized Studies.” Journal of Educational Psychology 66:688–701.
Saw, G., Schneider, B., Frank, K., Chen, I.C., Keesler, V. and Martineau, J., 2017. The Impact of Being Labeled as a Persistently Lowest Achieving School: Regression Discontinuity Evidence on Consequential School Labeling. American Journal of Education, 123(4), pp.585-613.
Scharfstein, D. A. I. 2002. “Generalized Additive Selection Models for the Analysis of Studies with Potentially Nonignorable Missing Data.” Biometrics 59:601–13.
VanderWeele, T.J. and Arah, O.A., 2011. Bias formulas for sensitivity analysis of unmeasured confounding for general outcomes, treatments, and confounders. Epidemiology (Cambridge, Mass.), 22(1), pp.42-52.
VanderWeele, T.J., 2010. Bias formulas for sensitivity analysis for direct and indirect effects. Epidemiology (Cambridge, Mass.), 21(4), p.540.
Wooldridge, J.M., 2010. Econometric analysis of cross section and panel data. MIT press.