
@article{razavi2021,
	title = {The {Future} of {Sensitivity} {Analysis}: {An} essential discipline for systems modeling and policy support},
	volume = {137},
	issn = {1364-8152},
	shorttitle = {The {Future} of {Sensitivity} {Analysis}},
	url = {https://www.sciencedirect.com/science/article/pii/S1364815220310112},
	doi = {10.1016/j.envsoft.2020.104954},
	abstract = {Sensitivity analysis (SA) is en route to becoming an integral part of mathematical modeling. The tremendous potential benefits of SA are, however, yet to be fully realized, both for advancing mechanistic and data-driven modeling of human and natural systems, and in support of decision making. In this perspective paper, a multidisciplinary group of researchers and practitioners revisit the current status of SA, and outline research challenges in regard to both theoretical frameworks and their applications to solve real-world problems. Six areas are discussed that warrant further attention, including (1) structuring and standardizing SA as a discipline, (2) realizing the untapped potential of SA for systems modeling, (3) addressing the computational burden of SA, (4) progressing SA in the context of machine learning, (5) clarifying the relationship and role of SA to uncertainty quantification, and (6) evolving the use of SA in support of decision making. An outlook for the future of SA is provided that underlines how SA must underpin a wide variety of activities to better serve science and society.},
	language = {en},
	urldate = {2022-10-18},
	journal = {Environmental Modelling \& Software},
	author = {Razavi, Saman and Jakeman, Anthony and Saltelli, Andrea and Prieur, Clémentine and Iooss, Bertrand and Borgonovo, Emanuele and Plischke, Elmar and Lo Piano, Samuele and Iwanaga, Takuya and Becker, William and Tarantola, Stefano and Guillaume, Joseph H. A. and Jakeman, John and Gupta, Hoshin and Melillo, Nicola and Rabitti, Giovanni and Chabridon, Vincent and Duan, Qingyun and Sun, Xifu and Smith, Stefán and Sheikholeslami, Razi and Hosseini, Nasim and Asadzadeh, Masoud and Puy, Arnald and Kucherenko, Sergei and Maier, Holger R.},
	month = mar,
	year = {2021},
	keywords = {Decision making, Machine learning, Mathematical modeling, Model robustness, Model validation and verification, Policy support, Sensitivity analysis, Uncertainty quantification},
	pages = {104954},
	file = {ScienceDirect Full Text PDF:/Users/sarahnarvaiz/Zotero/storage/MI6KWPHU/Razavi et al. - 2021 - The Future of Sensitivity Analysis An essential d.pdf:application/pdf;ScienceDirect Snapshot:/Users/sarahnarvaiz/Zotero/storage/HKD8ID46/S1364815220310112.html:text/html},
}

@article{cinelli2019,
    author = {Cinelli, Carlos and Hazlett, Chad},
    title = "{Making Sense of Sensitivity: Extending Omitted Variable Bias}",
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {82},
    number = {1},
    pages = {39-67},
    year = {2019},
    month = {12},
    abstract = "{We extend the omitted variable bias framework with a suite of tools for sensitivity analysis in regression models that does not require assumptions on the functional form of the treatment assignment mechanism nor on the distribution of the unobserved confounders, naturally handles multiple confounders, possibly acting non-linearly, exploits expert knowledge to bound sensitivity parameters and can be easily computed by using only standard regression results. In particular, we introduce two novel sensitivity measures suited for routine reporting. The robustness value describes the minimum strength of association that unobserved confounding would need to have, both with the treatment and with the outcome, to change the research conclusions. The partial R2 of the treatment with the outcome shows how strongly confounders explaining all the residual outcome variation would have to be associated with the treatment to eliminate the estimated effect. Next, we offer graphical tools for elaborating on problematic confounders, examining the sensitivity of point estimates and t-values, as well as ‘extreme scenarios’. Finally, we describe problems with a common ‘benchmarking’ practice and introduce a novel procedure to bound the strength of confounders formally on the basis of a comparison with observed covariates. We apply these methods to a running example that estimates the effect of exposure to violence on attitudes toward peace.}",
    issn = {1369-7412},
    doi = {10.1111/rssb.12348},
    url = {https://doi.org/10.1111/rssb.12348},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/82/1/39/49320681/jrsssb\_82\_1\_39.pdf},
}

@article{frank2000,
	title = {Impact of a {Confounding} {Variable} on a {Regression} {Coefficient}},
	volume = {29},
	issn = {0049-1241},
	url = {https://doi.org/10.1177/0049124100029002001},
	doi = {10.1177/0049124100029002001},
	abstract = {Regression coefficients cannot be interpreted as causal if the relationship can be attributed to an alternate mechanism. One may control for the alternate cause through an experiment (e.g., with random assignment to treatment and control) or by measuring a corresponding confounding variable and including it in the model. Unfortunately, there are some circumstances under which it is not possible to measure or control for the potentially confounding variable. Under these circumstances, it is helpful to assess the robustness of a statistical inference to the inclusion of a potentially confounding variable. In this article, an index is derived for quantifying the impact of a confounding variable on the inference of a regression coefficient. The index is developed for the bivariate case and then generalized to the multivariate case, and the distribution of the index is discussed. The index also is compared with existing indexes and procedures. An example is presented for the relationship between socioeconomic background and educational attainment, and a reference distribution for the index is obtained. The potential for the index to inform causal inferences is discussed, as are extensions.},
	number = {2},
	urldate = {2022-10-18},
	journal = {Sociological Methods \& Research},
	author = {Kenneth A. Frank},
	month = nov,
	year = {2000},
	note = {Publisher: SAGE Publications Inc},
	pages = {147--194},
	file = {SAGE PDF Full Text:/Users/sarahnarvaiz/Zotero/storage/DACJL8DW/FRANK - 2000 - Impact of a Confounding Variable on a Regression C.pdf:application/pdf},
}

@article{frank2007,
author = {Kenneth A. Frank and Kyung-Seok Min},
title ={10. Indices of Robustness for Sample Representation},
journal = {Sociological Methodology},
volume = {37},
number = {1},
pages = {349-392},
year = {2007},
doi = {10.1111/j.1467-9531.2007.00186.x},

URL = { 
        https://doi.org/10.1111/j.1467-9531.2007.00186.x
    
},
eprint = { 
        https://doi.org/10.1111/j.1467-9531.2007.00186.x
    
}
,
    abstract = { Social scientists are rarely able to gather data from the full range of contexts to which they hope to generalize (Shadish, Cook, and Campbell 2002). Here we suggest that debates about the generality of causal inferences in the social sciences can be informed by quantifying the conditions necessary to invalidate an inference. We begin by differentiating the target population into two sub-populations: a potentially observed subpopulation from which all of a sample is drawn and a potentially unobserved subpopulation from which no members of the sample are drawn but which is part of the population to which policymakers seek to generalize. We then quantify the robustness of an inference in terms of the conditions necessary to invalidate an inference if cases from the potentially unobserved subpopulation were included in the sample. We apply the indices to inferences regarding the positive effect of small classes on achievement from the Tennessee class size study and then consider the breadth of external validity. We use the statistical test for whether there is a difference in effects between two subpopulations as a baseline to evaluate robustness, and we consider a Bayesian motivation for the indices and compare the use of the indices with other procedures. In the discussion we emphasize the value of quantifying robustness, consider the value of different quantitative thresholds, and conclude by extending a metaphor linking statistical and causal inferences. }
}

@article{frank2008,
 ISSN = {01623737, 19351062},
 URL = {http://www.jstor.org/stable/30128050},
 abstract = {In addition to identifying and developing superior classroom teaching, the National Board for Professional Teaching Standards (NBPTS) certification process is intended to identify and cultivate teachers who are more engaged in their schools. Here the authors ask, "Does NBPTS certification affect the number of colleagues a teacher helps with instructional matters?" If so, this could enhance the influence of NBPTS-certified teachers and their contributions to their professional communities. Using sociometric data within 47 elementary schools from two states, the authors find that NBPTS-certified teachers were nominated more as providing help with instruction than non-NBPTS-certified teachers. From analyses using propensity score weighting, the authors then infer that NBPTS certification affects the number of colleagues a teacher helps with instructional matters. The authors then quantify the robustness of their inference in terms of internal and external validity, finding, for example, that any omitted confounding variable would have to have an impact six times larger than that of their strongest covariate to invalidate their inference. Therefore, the potential value added by NBPTS-certified teachers as help providers has policy and practice implications in an era when teacher leadership has risen to the fore as a critical force for school improvement.},
 author = {Kenneth A. Frank and Gary Sykes and Dorothea Anagnostopoulos and Marisa Cannata and Linda Chard and Ann Krause and Raven McCrory},
 journal = {Educational Evaluation and Policy Analysis},
 number = {1},
 pages = {3--30},
 publisher = {[American Educational Research Association, Sage Publications, Inc.]},
 title = {Does NBPTS Certification Affect the Number of Colleagues a Teacher Helps with Instructional Matters?},
 urldate = {2023-07-24},
 volume = {30},
 year = {2008}
}

@article{frank2013,
	title = {What {Would} {It} {Take} to {Change} an {Inference}? {Using} {Rubin}’s {Causal} {Model} to {Interpret} the {Robustness} of {Causal} {Inferences}},
	volume = {35},
	issn = {0162-3737},
	shorttitle = {What {Would} {It} {Take} to {Change} an {Inference}?},
	url = {https://doi.org/10.3102/0162373713493129},
	doi = {10.3102/0162373713493129},
	abstract = {We contribute to debate about causal inferences in educational research in two ways. First, we quantify how much bias there must be in an estimate to invalidate an inference. Second, we utilize Rubin?s causal model to interpret the bias necessary to invalidate an inference in terms of sample replacement. We apply our analysis to an inference of a positive effect of Open Court Curriculum on reading achievement from a randomized experiment, and an inference of a negative effect of kindergarten retention on reading achievement from an observational study. We consider details of our framework, and then discuss how our approach informs judgment of inference relative to study design. We conclude with implications for scientific discourse.},
	number = {4},
	urldate = {2022-10-18},
	journal = {Educational Evaluation and Policy Analysis},
	author = {Kenneth A. Frank and Spiro J. Maroulis and Minh Q. Duong and Benjamin M. Kelcey},
	month = dec,
	year = {2013},
	note = {Publisher: American Educational Research Association},
	pages = {437--460},
	file = {SAGE PDF Full Text:/Users/sarahnarvaiz/Zotero/storage/8NMAK5L4/Frank et al. - 2013 - What Would It Take to Change an Inference Using R.pdf:application/pdf},
}

@article{frank2021,
title = {Hypothetical case replacement can be used to quantify the robustness of trial results},
journal = {Journal of Clinical Epidemiology},
volume = {134},
pages = {150-159},
year = {2021},
issn = {0895-4356},
doi = {10.1016/j.jclinepi.2021.01.025},
url = {https://www.sciencedirect.com/science/article/pii/S0895435621000366},
author = {Kenneth A. Frank and Qinyun Lin and Spiro Maroulis and Anna S. Mueller and Ran Xu and Joshua M. Rosenberg and Christopher S. Hayter and Ramy A. Mahmoud and Marynia Kolak and Thomas Dietz and Lixin Zhang},
keywords = {Robustness of findings, Randomized controlled trials, Fragility, Case replacement, Statistical significance, Clinical importance},
abstract = {Objectives
We apply a general case replacement framework for quantifying the robustness of causal inferences to characterize the uncertainty of findings from clinical trials.
Study design and setting
We express the robustness of inferences as the amount of data that must be replaced to change the conclusion and relate this to the fragility of trial results used for dichotomous outcomes. We illustrate our approach in the context of an RCT of hydroxychloroquine on pneumonia in COVID-19 patients and a cumulative meta-analysis of the effect of antihypertensive treatments on stroke.
Results
We developed the Robustness of an Inference to Replacement (RIR), which quantifies how many treatment cases with positive outcomes would have to be replaced with hypothetical patients who did not receive a treatment to change an inference. The RIR addresses known limitations of the Fragility Index by accounting for the observed rates of outcomes. It can be used for varying thresholds for inference, including clinical importance.
Conclusion
Because the RIR expresses uncertainty in terms of patient experiences, it is more relatable to stakeholders than P-values alone. It helps identify when results are statistically significant, but conclusions are not robust, while considering the rareness of events in the underlying data.}
}

@article{xu2019,
author = {Ran Xu and Kenneth A. Frank and Spiro J. Maroulis and Joshua M. Rosenberg},
title ={konfound: Command to quantify robustness of causal inferences},
journal = {The Stata Journal},
volume = {19},
number = {3},
pages = {523-550},
year = {2019},
doi = {10.1177/1536867X19874223},

URL = { 
        https://doi.org/10.1177/1536867X19874223
    
},
eprint = { 
        https://doi.org/10.1177/1536867X19874223
    
}
,
    abstract = { Statistical methods that quantify the discourse about causal inferences in terms of possible sources of biases are becoming increasingly important to many social-science fields such as public policy, sociology, and education. These methods are also known as “robustness or sensitivity analyses”. A series of recent works (Frank [2000, Sociological Methods and Research 29: 147–194]; Pan and Frank [2003, Journal of Educational and Behavioral Statistics 28: 315– 337]; Frank and Min [2007, Sociological Methodology 37: 349–392]; and Frank et al. [2013, Educational Evaluation and Policy Analysis 35: 437–460]) on robustness analysis extends earlier methods. We implement these recent developments in Stata. In particular, we provide commands to quantify the percent bias necessary to invalidate an inference from a Rubin causal model framework and the robustness of causal inferences in terms of correlations associated with unobserved variables. }
}

@article{frank2023,
title = "Quantifying the robustness of causal inferences: Sensitivity analysis for pragmatic social science",
abstract = "Social scientists seeking to inform policy or public action must carefully consider how to identify effects and express inferences because actions based on invalid inferences may not yield the intended results. Recognizing the complexities and uncertainties of social science, we seek to inform inevitable debates about causal inferences by quantifying the conditions necessary to change an inference. Specifically, we review existing sensitivity analyses within the omitted variables and potential outcomes frameworks. We then present the Impact Threshold for a Confounding Variable (ITCV) based on omitted variables in the linear model and the Robustness of Inference to Replacement (RIR) based on the potential outcomes framework. We extend each approach to include benchmarks and to fully account for sampling variability represented by standard errors as well as bias. We exhort social scientists wishing to inform policy and practice to quantify the robustness of their inferences after utilizing the best available data and methods to draw an initial causal inference.",
keywords = "Causal inference, Pragmatic sociology, Sensitivity analysis",
author = "{Frank, Kenneth A. and Lin, Qinyun and Xu, Ran  and Maroulis, Spiro  and Mueller, Anna}",
note = "Funding Information: This work was supported by grant R305D220022 from the US Institute for Education Sciences. Publisher Copyright: {\textcopyright} 2022 Elsevier Inc.",
year = "2023",
month = feb,
doi = "10.1016/j.ssresearch.2022.102815",
language = "English (US)",
volume = "110",
journal = "Social Science Research",
issn = "0049-089X",
publisher = "Academic Press Inc.",
}

@misc{frank2022, 
title={Improving Oster’s δ*: Exact calculation for the coefficient of proportionality without subjective specification of a baseline model},
url={https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4305243}, 
journal={SSRN}, 
author={Frank, Kenneth A. and Lin, Qinyun and Maroulis, Spiro and Dai, Shimeng and Jess, Nicole and Lin, Hung-chang and liu, Yuqing and Maestrales, Sarah and Searle, Ellen and Tait, Jordan}, 
year={2022}, 
month={Dec}} 
